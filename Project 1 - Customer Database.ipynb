{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Customer Database\n",
    "**This is the first of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-10-13, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements, and you should not modify the structure of the notebook**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Your team has been hired by the company X as data scientists. X makes gadgets for a wide range of industrial and commercial clients.\n",
    "\n",
    "As in-house data scientists, your teams first task, as per request from your new boss, is to optimize business operations. You have decided that a good first step would be to analyze the companys historical sales data to gain a better understanding of where profit is coming from. It may also reveal some low hanging fruit in terms of business opportunities.\n",
    "\n",
    "To get started, you have called the IT department to get access to the customer and sales transactions database. To your horror you've been told that such a database doens't exist, and the only record of sales transactions is kept by John from finance in an Excel spreadsheet. So you've emailed John asking for a CSV dump of the spreadsheet...\n",
    "\n",
    "In this project you need to clean the data you got from John, enrich it with further data, prepare a database for the data, and do some data analysis. The project is comprised of five parts. They are intended to be solved in the order they appear, but it is highly recommended that you read through all of them and devise an overall strategy before you start implementing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleaning the data\n",
    "John has emailed you the following link to the CSV dump you requested.\n",
    "\n",
    "- [transactions.csv](https://raw.githubusercontent.com/patrickcording/02807-comp-tools/master/docker/work/data/transactions.csv)\n",
    "\n",
    "It seems as though he has been a bit sloppy when keeping the records. \n",
    "\n",
    "In this part you should:\n",
    "- Explain what the data is\n",
    "- Clean it to prepare it for inserting into a database and doing data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "In this part we clean the data from the csv file. This is done by evaluating each column/feature by itself. E.g. we start by evaluating the 'Part' column of the data. \n",
    "\n",
    "When evaluating each feature, we start with markdown cell, where we describe how we have explored the data and which problems we have found. Hereafter, we have a code cell where we fix the errors using python.\n",
    "\n",
    "Primarily we have ushed bash commands to examine the data and python to fix it. However, in few cases we examined the data directly in python by using the print command (this code is removed from code cells now). \n",
    "\n",
    "Even though we evaluate the data column by column, we found that errors regarding the columns 'company', 'country' and 'city' have a relation. E.g. if one of these columns are missing the value can often be inferred by knowing the two others. Therefore, these three columns are evaluated together.\n",
    "\n",
    "Choosing this approach, we deem that the report and code should be easier to read. \n",
    "However, this solution requires us to iterate over the data for each column we want to fix something in.\n",
    "This is unnecessary, and lead to a slower solution when cleaning the data.\n",
    "\n",
    "However, since we only execute the code once (after the data is cleaned, it is clean) we have deemed the readability more important than the performance. \n",
    "\n",
    "Initially, we start by importing necessary libaries and data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import requests as req\n",
    "import sqlite3\n",
    "\n",
    "#we assume transactions.csv is placed in same directory as this project\n",
    "df = pd.read_csv(\"transactions.csv\", encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part\n",
    "The Part feature seems to be a partnumber (string). To investigate the purity and nature of the data, the following command revealed a good indication:\n",
    "\n",
    "'cut -d , -f 1 transactions.csv | sort | uniq -c'\n",
    "\n",
    "The command shows:\n",
    "    - majority of part numbers were used around 200 times\n",
    "    - 10 records countained an empty part number\n",
    "    - some part numbers are empty\n",
    "    - The part numbers which were not empty, had format: 4 or 5 digits followed by '-' followed by 3 or 4 digits\n",
    "    - In total there are 100 different part numbers (excluding the empty part number)\n",
    "\n",
    "By the nature of a part number, we cannot infer a part number from remaining data. Thus, we are not able to provide correct part numbers for rows with empty part numbers. It is not possible to infer the part number from the price, since it seems like rows with the same part number don't necessarily have the same price. Therefore, no cleanup is made for the 'Part' feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company\n",
    "This seems to be a company name (string). To analyse the purity/nature following command was used:\n",
    "\n",
    "'cut -d , -f 2 transactions.csv | sort | uniq -c'\n",
    "\n",
    "The command revealed: \n",
    "     - 35 different companies exist. However, some of these are spelling errors (e.g. 'Ntagz' for 'Ntags')\n",
    "     - Since the command displays all different companies in differnet order, it is fairly easy to see spelling errors\n",
    "     - Most appearing company is 'Thoughtmix' which appears 2795 times (spelling errors of Thoughtmix not included)\n",
    "     - Apart from spelling errors there seem to be 3 outliers: '-', 'a', 'aa' which all appear a single time\n",
    "     - Spelling errors only appear in a few rows, e.g. error of 'Thoughtmix': 'Thoughtmixz' only appeared once\n",
    "\n",
    "### Country\n",
    "This seems to be the country, where the company is placed (string). To analyse the purity/nature following command was used:\n",
    "\n",
    "'cut -d , -f 3 transactions.csv | sort | uniq -c'\n",
    "\n",
    "The command revealed:\n",
    "    - 14 different countries were listed (3 were spelling errors, e.g. 'US' in stead of 'United States' \n",
    "    - The 'empty' country appeared 2171 times so it seems like John forgot to note the company many times\n",
    "    - One spelling error is 'Germany' listed in danish: 'Tyskland'\n",
    "    - The spelling errors and the list of 'Tyskland' only appears in few rows (<5) while countries typically appear in > 1000 rows (except from Japan and Netherlands)\n",
    "\n",
    "### City \n",
    "This seems to be the city, where the company is placed (string). To analyse the purity/nature following command was used:\n",
    "\n",
    "'cut -d , -f 4 transactions.csv | sort | uniq -c'\n",
    "\n",
    "The command reveals:\n",
    "    - This gives 31 different cities\n",
    "    - The whitespace city appear 33 times (John was a bit better at noting cities down)\n",
    "    - There seems to be no spelling mistakes, however the cities 'Vila Fria' and 'Monção' only appear in a single row\n",
    "\n",
    "\n",
    "### Fixing Issues for Company/Country/Cities\n",
    "To fix issues for column: 'company', 'country', 'city' (including many empty countries), following command is useful:\n",
    "\n",
    "'cut -d , -f 2-4 transactions.csv | sort | uniq -c'\n",
    "\n",
    "A part of the result for this query is: \n",
    "     - 2    Brainsphere,,\n",
    "     - 122  Brainsphere,,Braga\n",
    "     - 1    Brainsphere,Portuga,Braga\n",
    "     - 2    Brainsphere,Portugal,\n",
    "     - 1114 Brainsphere,Portugal,Braga\n",
    "     - 1    Brainsphere,Portugal,Monção\n",
    "     \n",
    "Here, it can easily be seen that the correct location for Brainsphere is : 'Brainsphere,Portugal,Braga' (also based on number of appearences). This can be used to infer missing values/correct false data - even for the line only indicating the company (this only appears two times, so it doesn't seem realisticly that another officy should exist). This can also be used to fix the line containing 'Monção', since it seems like a mistake, to only be used once.\n",
    "\n",
    "The same method can be used for the remaining companies/countries/cities, so empty data can be inferred. It is even possible to infer the real data for the three company outliers '-', 'a', 'aa'. Both these lines list cities, which are only used by one company. Thus the company for these lines can be set to that exact comapny.\n",
    "\n",
    "The above comman also shows that every company is only located in one city, but for 'Flipstorm' which is both located in Athens and Nanterre.\n",
    "\n",
    "For more details see code comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##iterate over all rows to fix errors found examining the data in bash (described above)    \n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    #lines including Avaveo only have missing/wrongcountry\n",
    "    if (row['company'] == 'Avaveo' and row['country']!='France'):\n",
    "        row['country'] = 'France'\n",
    "    \n",
    "    #lines including 'Brinasphere' can both have missing/wrong country and missing/wrong city\n",
    "    elif (row['company'] == 'Brainsphere'):\n",
    "        if(row['country']!= 'Portugal'):\n",
    "            row['country'] = 'Portugal'\n",
    "        if(row['city']!='Braga'):\n",
    "            row['city'] = 'Braga'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']== 'Bubblemix' and row['country'] !='Japan'):\n",
    "        row['country'] = 'Japan'\n",
    "        \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Buzzbean' and row['country'] != 'Germany'):\n",
    "        row['country'] = 'Germany'\n",
    "        \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Chatterbridge' and row['country']!= 'Spain'):\n",
    "        row['country']='Spain'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Eimbee' and row['country']!= 'France'):\n",
    "        row['country']='France'\n",
    "\n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Flipstorm'):\n",
    "        if(row['city'] == 'Athens' and row['country'] != 'Greece'):\n",
    "            row['country']='Greece'\n",
    "        if(row['city'] == 'Nanterre' and row['country'] != 'France'):\n",
    "            row['country']='France'\n",
    "   \n",
    "    #lines only have missing/wrong country    \n",
    "    elif(row['company']=='Gabcube' and row['country']!= 'Portugal'):\n",
    "        row['country']='Portugal'    \n",
    "\n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Gabtune' and row['country']!= 'France'):\n",
    "        row['country']='France'\n",
    "\n",
    "    #lines only have missing/wrong country    \n",
    "    elif(row['company']=='Gevee' and row['country']!= 'France'):\n",
    "        row['country']='France'\n",
    "\n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Innojam' and row['country']!= 'Netherlands'):\n",
    "        row['country']='Netherlands'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Kanoodle'):\n",
    "        if(row['country']!= 'Netherlands'):\n",
    "            row['country']='Netherlands'\n",
    "        if(row['city']!='Niihama'):\n",
    "            row['city']='Niihama'\n",
    "    \n",
    "    #this line is to fix single row which listed Lajo as Laj0\n",
    "    elif(row['company'] == 'Laj0'):\n",
    "        row['company'] = 'Lajo'\n",
    "    \n",
    "    #lines only have missing/wrong country    \n",
    "    elif(row['company']=='Lajo' and row['country']!= 'Greece'):\n",
    "        row['country']='Greece'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company'] == 'Ntagz'):\n",
    "        row['company'] = 'Ntags'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Ntags'):\n",
    "        if(row['country']!= 'Portugal'):\n",
    "            row['country']='Portugal'\n",
    "        if(row['city']!='Lisbon'):\n",
    "            row['city']='Lisbon'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Realpoint' and row['country']!= 'Portugal'):\n",
    "        row['country']='Portugal'\n",
    "\n",
    "    #lines only have missing/wrong country    \n",
    "    elif(row['company']=='Thycero' and row['country']!= 'France'):\n",
    "        row['country']='France'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Riffpath' and row['country']!= 'Greece'):\n",
    "        row['country']='Greece'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Roodel' and row['country']!= 'Portugal'):\n",
    "        row['country']='Portugal'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Shufflebeat'):\n",
    "        if(row['country']!= 'Portugal'):\n",
    "            row['country']='Portugal'\n",
    "        if(row['city']!='Porto'):\n",
    "            row['city']='Porto'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company']=='Tagtune' and row['country']!= 'Switzerland'):\n",
    "        row['country']='Switzerland'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Teklist'):\n",
    "        if(row['country']!= 'Netherlands'):\n",
    "            row['country']='Netherlands'\n",
    "        if(row['city']!='Arnhem'):\n",
    "            row['city']='Arnhem'\n",
    "\n",
    "    #lines only have missing/wrong country        \n",
    "    elif(row['company'] == 'Thoughtmixz'):\n",
    "        row['company'] = 'Thoughtmix'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Thoughtmix'):\n",
    "        if(row['country']!= 'Portugal'):\n",
    "            row['country']='Portugal'\n",
    "        if(row['city']!='Amadora'):\n",
    "            row['city']='Amadora'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Twitterbeat'):\n",
    "        if(row['country']!= 'France'):\n",
    "            row['country']='France'\n",
    "        if(row['city']!='Annecy'):\n",
    "            row['city']='Annecy'\n",
    "    \n",
    "    #lines only have missing/wrong country        \n",
    "    elif(row['company']=='Voomm' and row['country']!= 'France'):\n",
    "        row['country']='France'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Wordify'):\n",
    "        if(row['country']!= 'United States'):\n",
    "            row['country']='United States'\n",
    "        if(row['city']!='New York'):\n",
    "            row['city']='New York'\n",
    "\n",
    "    #lines can both have missing/wrong country and missing/wrong city        \n",
    "    elif(row['company']=='Yozio'):\n",
    "        if(row['country']!= 'Greece'):\n",
    "            row['country']='Greece'\n",
    "        if(row['city']!='Patras'):\n",
    "            row['city']='Patras'\n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Zoonder'):\n",
    "        if(row['country']!= 'United States'):\n",
    "            row['country']= 'United States'\n",
    "        if(row['city']!='Boston'):\n",
    "            row['city']='Boston'\n",
    "    \n",
    "    #lines only have missing/wrong country\n",
    "    elif(row['company'] == 'Zooxo.'):\n",
    "        row['company'] = 'Zooxo'        \n",
    "    \n",
    "    #lines can both have missing/wrong country and missing/wrong city\n",
    "    elif(row['company']=='Zooxo'):\n",
    "        if(row['country']!= 'United Kingdom'):\n",
    "            row['country']= 'United Kingdom'\n",
    "        if(row['city']!='London'):\n",
    "            row['city']='London'\n",
    "    \n",
    "    #We set company for typo'ed companies based on their location (NY and Boston)\n",
    "    #Since there is only one company we sell to in new york and one company in Boston\n",
    "    elif(row['company'] == ' -'): #this line lists Boston. Only company in Boston is Zoonder\n",
    "        row['company'] = 'Zoonder'\n",
    "        \n",
    "    elif(row['company'] == ' a'): #this line lists New York. Only company in Boston is Wordify\n",
    "        row['company'] = 'Wordify'\n",
    "        \n",
    "    elif(row['company'] == 'aa'): #this line lists New York. Only company in Boston is Wordify\n",
    "        row['company'] = 'Wordify'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price\n",
    "This column indicates the price for the transaction. To investigate the purity/nature of the data a command similar to the others were executed: \n",
    "\n",
    "'cut -d , -f 5 transactions.csv | sort | uniq -c | wc -l'\n",
    "\n",
    "However, this shows that more than 19000 different price values exist. Therefore, we took another approach and wanted to inspect which char the prices started with using command:\n",
    "\n",
    "'cut -d , -f 5 transactions.csv | cut -c 1-1 | sort | uniq -c'\n",
    "\n",
    "The command shows that many lines start with the monetary values $, £, ¥ and values from 1-9. Also many lines start with a '-', probably indicating negative values. A few mistakes seemed to be in there: 'p', ' ', and 'n'. These values cound be further inspected by commands like: \n",
    "\n",
    "'cut -d , -f 5 transactions.csv | grep n'\n",
    "\n",
    "This revealed following problems with data: \n",
    "    - two rows have value 'void'\n",
    "    - Three rows have value 'na'\n",
    "    - five lines have value '-'\n",
    "    - negatives lines appears in euros with trailing € sign:  e. g. '-121.28€' while dollars and rest have sign as prefix: '$-22.66'.\n",
    "    \n",
    "The last point is also the case for positive values. \n",
    "\n",
    "We now investigate two possible errors: no currency sign or multiple currency sign. The no currency sign was investigated using:\n",
    "\n",
    "\"cut -d , -f 5 transactions.csv | egrep -v '[$£¥€]'\"\n",
    "\n",
    "Which revealed preivoulsy listed errors and four prices not having any currency sign. \n",
    " \n",
    "\"cut -d , -f 5 transactions.csv | egrep '[$£¥€].*[$£¥€]'\" - the line contains dollar sign to, but when this box is evaluated it is not showing.\n",
    "\n",
    "The command Revealed that no line had a price with more than one currency sign\n",
    "\n",
    "We chose to set price of rows with values 'na'/'void'/'-' to 0. Another approach would have been to set it to median or average value. \n",
    "\n",
    "Examining the use of the currencies we can see that € appear far more often than other currencies. Therefore, we have chosen € as currency for the value without any currencies. \n",
    "\n",
    "These solutions are implemented below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows(): #iterate over all rows\n",
    "    if (row['price'] == 'na' or row['price'] == '-' or row['price'] == 'void' or not isinstance(row['price'],str)):\n",
    "        row['price']=0 #find outliers and set their value to 0\n",
    "    else:\n",
    "        contain_currency = 0 #find values without currency\n",
    "        if (not '€' in row['price']): \n",
    "            contain_currency = 1\n",
    "        if ('$' in row['price']):\n",
    "            contain_currency = 1\n",
    "        if ('£' in row['price']):\n",
    "            contain_currency = 1\n",
    "        if ('¥' in row['price']):\n",
    "            contain_currency = 1\n",
    "        if(contain_currency==0):\n",
    "            row['price']=row['price']+'€' #assume default value is € if none is declared "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date\n",
    "This seems to be the date for the transaction, consisting of a date and a specific time (hours, minutes, seconds).\n",
    "\n",
    "\n",
    "The following query was made to examine the data:\n",
    "\n",
    "\n",
    "'cut -d , -f 6 transactions.csv | sort | uniq -c | head'\n",
    "\n",
    "The query revealed one date formatted as dd/mm/yyyy and all remaining rows had format 'yyyy:mm:dd HH:MM:SS'. All results for tail command also had latter format. To inspect time parameter further, we used query:\n",
    " \n",
    "'cut -d , -f 6 transactions.csv | cut -d ' ' -f 2 | sort | uniq -c | head' - also with tail\n",
    "\n",
    "From the query it seems like almost every date value also include timestamp. 9 Rows had timestamp value '' thus having no timestamp. The query also showed that the timestamps were disitrbutted between 00:00:00 and 23:59:59, as expected. \n",
    "\n",
    "To conclude second value in timestamp e.g. ':HH:' is in fact hours we insepcted data using query:\n",
    "\n",
    "'cut -d , -f 6 transactions.csv | cut -d ' ' -f 1 | cut -d '-' -f 2 | sort | uniq -c '\n",
    "\n",
    "To clean up the date we add a timestamp (00:00:00) to columns without any and convert one row with value '10/04/2017' into correct format. \n",
    "\n",
    "When importing the data into python date_times (part 2) we got exception for two values, which had day '32' listed. Since no month has date '32' we converted these to day 30, since both were in June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():#iterate all rows\n",
    "    if (row['date'].startswith('10/04/2017')): \n",
    "        row['date']='2017-04-10' #translate single date with wrong format to correct date format\n",
    "    if (row['date'].startswith('2016-06-32 07:22:28')):\n",
    "        row['date']='2016-06-30 07:22:28' #change date with too large date value\n",
    "    if (row['date'].startswith('2016-06-32 08:08:48')):\n",
    "        row['date']='2016-06-30 08:08:48'#change date with too large date value\n",
    "\n",
    "    try: #try read lines with formay yyyy-mm-dd and convert them to format yyyy-mm-dd HH:MM:SS\n",
    "        #some dates without HH:MM:SS have trailing whitespace, so we strip here\n",
    "        date = row['date'].strip() #strip for mig\n",
    "        t = datetime.strptime(date, \"%Y-%m-%d\") #\n",
    "        row['date']=t.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except: #in case the line did not have formay yyyy-mm-dd we ensure it has format yyyy-mm-dd HH:MM:SS\n",
    "        try: #This let us print out an error message in case there exist a timestamp not matching desired format\n",
    "            t = datetime.strptime(row['date'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        except:\n",
    "            print('Error converting time for transaction ' +  row['company'] + \" \" + row['date']) #used for debugging\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Enriching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task for a data scientists is to combine or enrich data from internal sources with data available from external sources. The purpose of this can be either to fix issues with the data or to make it easier to derive insights from the data.\n",
    "\n",
    "In this part you should enrich your data with data from at least one external source. You may look to part 4 for some  inspiration as to what is required. Your solution should be automated, i.e., you can not ask the reader of your notebook to download any data manually. You should argue why and what you expect to achieve by the enrichments you are doing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decribe required enrichment\n",
    "In section 4 we have to compare the profit between companies. To do so, we should calculate all prices to use same currency, such that price of every transaction can be compared with the price of any other transaction.\n",
    "\n",
    "Furthermore, we build on the formatting of the price column established in last exercise. Here the formatting was \"X.Y\" where the string also contained a currency symbol. Apart from declaring everything as euro, we choose to store the value as an int in the database declaring the price as cents. \n",
    "\n",
    "E.g. if exchange rate from usd to eur is 2:1 and we receive '$2.34' we first calculate it to '1.17' EUR and then save '117' as value. The reason for doing this is so we can store the value in the database as an integer. \n",
    "\n",
    "As mentioned, the reason for this transformation is to easier compare prices between companies in section 4. While computing this transformation the code became a bit complicated so we decided to make use of functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error displaying currency:  None date:  2016-04-15 06:51:38 datetime  2016-05-05 06:51:38\n",
      "Error displaying currency:  None date:  2016-04-15 08:27:18 datetime  2016-05-05 08:27:18\n",
      "Error displaying currency:  None date:  2016-04-21 02:54:36 datetime  2016-05-11 02:54:36\n",
      "Error displaying currency:  None date:  2016-04-21 04:07:31 datetime  2016-05-11 04:07:31\n"
     ]
    }
   ],
   "source": [
    "#invoke rest api for exchange rates\n",
    "resp = req.get(\"https://api.exchangeratesapi.io/history?start_at=2016-01-02&end_at=2019-05-14&symbols=USD,GBP,JPY\")\n",
    "\n",
    "# Read it as json\n",
    "json = resp.json()\n",
    "# Read relevant part of json\n",
    "rate = json['rates']\n",
    "\n",
    "# make a data frame (cdf: currency data frame)\n",
    "cdf = pd.DataFrame.from_dict(rate)\n",
    "\n",
    "#This functinon calculates the exchange rate between a specified currency and EUR on a specified date\n",
    "#The function tries to obtain conversion ratio from exact day. If the conversion ratio is unavailable for that date\n",
    "#(e.g. weekends) we try the next day, then next next day, ... up til 20 days \n",
    "def get_eur_exchange_ratio(currency, date):\n",
    "    date_time_obj = datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\")#get date time object using part 1 format\n",
    "    i=0 #variable to control how many days we look forward\n",
    "    exchange_ratio = 0\n",
    "    while (exchange_ratio == 0 and i<20): #loop while we have not found exchange ratio and haven't tried i times yet\n",
    "        try: #try fetch currency using currency data frame (from rest)\n",
    "            exchange_ratio = cdf.at[currency,date_time_obj.strftime('%Y-%m-%d')]\n",
    "            return exchange_ratio \n",
    "        except: #if we fail, increase date time object by one day and loop control variable by 1\n",
    "            date_time_obj = date_time_obj + timedelta(1); #try next day\n",
    "            i+=1\n",
    "    #print for debug if we could not find exchange ratio    \n",
    "    print(\"Error displaying currency: \", currency, \"date: \", date, 'datetime ', date_time_obj)\n",
    "    return -1            \n",
    "\n",
    "#This function return the eur price given specified currency, value and date\n",
    "#currency has to be declared in dateframe format, e.g. 'USD'\n",
    "#original_value is here expected in cents (int)\n",
    "def get_eur_price(currency,original_value,date): \n",
    "    conversion_rate = get_eur_exchange_ratio(currency, date) #get conversion rate from data frame \n",
    "    eur_value = original_value/conversion_rate #apply conversion rate to get eur cent value\n",
    "    return int(round(eur_value))     #We round to ensure we get to nearest full 'cent'\n",
    "\n",
    "#This function gives string value, that currency is listed as in data frame for exchange rates\n",
    "#e.g. € is listed as 'EUR'\n",
    "def get_currency_string_from_price_string(price_string):\n",
    "    if('€' in price_string) :\n",
    "        return 'EUR'\n",
    "    elif('$' in price_string) : \n",
    "        return 'USD'\n",
    "    elif('£' in price_string): \n",
    "        return 'GBP'\n",
    "    elif('¥' in price_string):     #ASSUMED THAT ¥ is Japanese Yen \n",
    "        return 'JPY'\n",
    "\n",
    "#convert from 'euro.cents' format to two last digits in int declaring cents and any digit before than declaring euros\n",
    "#e.g. '2732' is interpreted as 27 euro and 32 cents\n",
    "def get_price_as_int(price):\n",
    "    #print('_: ' + price)\n",
    "    pricestrings = price.split('.') #divide between euros and cents\n",
    "    priceint = int(pricestrings[0]) * 100 #multiply euro value by 100 \n",
    "    #print('1: ' + str(priceint))\n",
    "    if (pricestrings[0].startswith('-')): \n",
    "        priceint -= int(pricestrings[1]) #if eur value is negative we need to subtract the cents from current value\n",
    "    else:\n",
    "        priceint += int(pricestrings[1])\n",
    "    #print('2: ' + str(priceint))\n",
    "    return priceint\n",
    "   \n",
    "\n",
    "#this loops iterates rows in dataframe from csv\n",
    "#for each row we exchange price value to EUR and save it back to dataframe as an int declaring eur as cents\n",
    "#e.g. if exchange rate from usd to eur is 2:1 and we receive '$2.34' we save '117'\n",
    "translation_table = dict.fromkeys(map(ord, '$€£¥'), None)\n",
    "for index, row in df.iterrows():\n",
    "    if (row['price']==0): #we ignore rows equal to zero. They have no currency and are already represented as eur cents\n",
    "        continue\n",
    "        \n",
    "    currency_string = get_currency_string_from_price_string(row['price']) #get currency string used in data frame object\n",
    "    price_without_currency = row['price'].translate(translation_table) #Remove currency symbol/char from price\n",
    "    price_int = get_price_as_int(price_without_currency) #convert to int, cent format\n",
    "    \n",
    "    if(currency_string == 'EUR'): #if already in EUR currency , we just use value right away we need no exchange \n",
    "        eur_price = price_int \n",
    "    else: #If not in EUR we exchange using dataframe from external api\n",
    "        eur_price = get_eur_price(currency_string, price_int, row['date']) #Calculate price in EUR\n",
    "    row['price'] = eur_price #save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Creating a database\n",
    "Storing data in a relational database has the advantages that it is persistent, fast to query, and it will be easier access for other employees at Weyland-Yutani.\n",
    "\n",
    "In this part you should:\n",
    "- Create a database and table(s) for the data\n",
    "- Insert data into the tables\n",
    "\n",
    "You may use SQLite locally to do this. You should argue why you choose to store your data the way you do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Description of Solution\n",
    "Til 3'eren\n",
    "\n",
    "### Argumentation of the chosen solution\n",
    "We chose to implement the database as one table called transactions. We considered storing two tables: one as company_id and one with transaction_ids. The advantage of this is that it saves space. We would then have a table _transactions_ with five columns (transaction_id (UNIQUE) part, price, datetime, company_id) and a table _companies_ with four columns (company_id (UNIQUE), company_name, company_country, company_city). Then we realized that there is a company with multiple locations, which would result in either: Lose of data (if we stored companies with just one location) or a many to many relation table to store references between a company table and a city table (4 tables in total) _companies_ table; _company_location_. Both of the solutions with an extra table would save some space as we only store a company(location)id once and then look it up when we search for a company in the main table _transactions_. However we chose only to implement it in one table due to the following: \n",
    "\n",
    "By implementing everything in one table we do use some more space, but we save some time in the implementing of the database. It is much easier to store data in the table we chose to implement. Both due to the fact that we only have one table, but it is also less complex to understand. This is an advantage if someone else has to maintain the database afterwards.\n",
    "Another bennefit is that the queries to the database are easier to create. The demands from the boss are reletively easy to query from the database. For example by only having one table, we do not have to do joins. This is an advantage if a less technical employee would have to use the database in the future. \n",
    "\n",
    "Furthermore, the advantage we get from storing it in two tables are mostly that we get less space. However, we don't think that with the given data it is much of a problem. It might be a better idea if the queries that the boss wanted were more complex, it is however hard to foresee the type of queries that the boss would want. Thus it is difficult to make a universal design that supports every possible wanted query. \n",
    "\n",
    "The solution implemented is therefore a single table _transactions_ with the columns: transaction_id, part, price, date_time, compamny_name, company_country and company_city. By storing it this way, we get simple SQL queries to insert the data. We get (relatively) simple SQL queries to query each company and what we've sold to them in a given time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to the database and create tables\n",
    "conn = sqlite3.connect('weylandYutani.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS transactions(transaction_id INTEGER PRIMARY KEY AUTOINCREMENT, part INTEGER, price INTEGER, date_time TEXT, company_name TEXT, company_country TEXT, company_city TEXT)')\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "## Import data into the table\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    part = row['part']\n",
    "    companyName = row['company']\n",
    "    companyCountry = row['country']\n",
    "    companyCity = row['city']\n",
    "    price = row['price']\n",
    "    date_time = row['date']\n",
    "    c.execute(\"INSERT INTO transactions(part, price, date_time,company_name, company_country, company_city) VALUES (?, ?, ?, ?, ?, ?)\", (part,price,date_time,companyName,companyCountry,companyCity))\n",
    "    conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Analyzing the data\n",
    "You are now ready to analyze the data. Your goal is to gain some actionable business insights to present to your boss. \n",
    "\n",
    "In this part, you should ask some questions and try to answer them based on the data. You should write SQL queries to retrieve the data. For each question, you should state why it is relevant and what you expect to find.\n",
    "\n",
    "To get you started, you should prepare answers to the following questions. You should add more questions.\n",
    "#### Who are the most profitable clients?\n",
    "Knowing which clients that generate the most revenue for the company will assist your boss in distributing customer service ressources.\n",
    "\n",
    "#### Are there any clients for which profit is declining?\n",
    "Declining profit from a specific client may indicate that the client is disatisfied with the product. Gaining a new client is often much more work than retaining one. Early warnings about declining profit may help your boss fighting customer churn.\n",
    "\n",
    "\n",
    "Remember, you are taking this to your new boss, so think about how you present the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##The following plot shows each company and the profit that our company earned over time. \n",
    "df4 = pd.read_sql_query('SELECT price, date_time, company_name FROM transactions', conn)\n",
    "\n",
    "companyNames = []\n",
    "for index, row in df4.iterrows():\n",
    "    if (row['company_name'] not in companyNames):\n",
    "        companyNames.append(row['company_name'])\n",
    "listOfCompanyTotal = []\n",
    "listOfCompanyDevelopment = []\n",
    "for companyName in companyNames:\n",
    "    # The SQL query we need for 4.1\n",
    "    c.execute(f\"SELECT sum(price), company_name FROM transactions WHERE company_name='{companyName}' ORDER BY company_name\")\n",
    "    listOfCompanyTotal.append(c.fetchone())\n",
    "    # We need for each company the development of price over time\n",
    "    # Here it is after the 2018-06-01 20:05:00\n",
    "    c.execute(f\"SELECT price, date_time FROM transactions WHERE date_time>'2018-06-01 20:05:00' AND company_name='{companyName}' \")\n",
    "    listOfCompanyDevelopment.append(c.fetchall())\n",
    "\n",
    "\n",
    "# Define the names for the plot for 4.1    \n",
    "new_name = {\n",
    "    0: 'profit',\n",
    "    1: 'company name'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# The following plot show the answer to the question: \n",
    "#.   Who are the most profitable clients?\n",
    "#.   It is a plot over the companies in the x-axis and the total sum of the price for that given company on the y-axis\n",
    "df4_1 = pd.DataFrame.from_dict(listOfCompanyTotal)\n",
    "df4_1.rename(columns=new_name, inplace=True)\n",
    "df4_1.plot(kind='bar', x=\"company name\")\n",
    "\n",
    "new_name2 = {\n",
    "    0:'price',\n",
    "    1:'date_time'\n",
    "}\n",
    "# The following plot show the answer to the question: \n",
    "#     Are there any clients for which profit is declining?\n",
    "#     Somewhat anyways. The list \"listOfCompanyDevelopment\" shows the development of each company over time. \n",
    "#   It is stored such that when quering listOfCompanyDevelopment[3] you get the 3rd companys development over time. \n",
    "#.   In the SQL statement above you can change the date_time for which the query should run\n",
    "df4_2 = pd.DataFrame.from_dict(listOfCompanyDevelopment[3])\n",
    "df4_2.rename(columns=new_name2, inplace=True)\n",
    "df4_2.head(100)\n",
    "df4_2.plot(x='date_time')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Performance\n",
    "Your boss is very impressed with what you have achieved in less than two weeks, and he would like to take your idea of storing the customer and sales data in a relational database to production. However, John is concerned that the solution will not scale. His experience is telling him that you will see many occurrences of the following queries.\n",
    "\n",
    "- Show all sales to company X between time $t_1$ and time $t_2$\n",
    "- Show the latest X sales in the database\n",
    "- Show total sales per company per day\n",
    "\n",
    "Show that Johns concern is not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is true that with increasing number of records in the database the query time will increase. However we can index the database using the index commands. We could forexample to support the queries described above, index on companies and date time. Thus the queries would be significantly faster, even with large databases. \n",
    "\n",
    "The indexes can be made on any column that the boss would want. This would speed up the queries on that given data column.\n",
    "\n",
    "Even maintaining the database would not be slower either. If the database indexing is implemented using beta-epsilon trees, we get insertions in $O(\\frac{log_B N}{\\epsilon B^{\\epsilon}})$, where B is the block size and N is the size of the data, and $\\epsilon$ is the relation between the degree of a node and the buffersize of the buffers that contain the updates. Where as if we would just use B-trees it would be $O(log_B N)$. Thus we get significantly better update times. The result is that we can actually maintain a database, while still being able to keep multiple index over the database. (See [An Introduction to Bε-trees and Write-Optimization](https://www.usenix.org/publications/login/oct15/bender)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
